import pandas as pd
import numpy as np
import yfinance as yf
import requests
from tqdm import tqdm
import matplotlib.pyplot as plt


import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.optimize import minimize
from scipy.stats import norm, chi2

def kupiec_test(hit_series, alpha):
    """
    hit_series: array-like de 0/1 (1 = VaR violado)
    alpha: nível do VaR (ex: 0.05 para 95%)
    """
    n = len(hit_series)
    x = np.sum(hit_series)  # número de violações
    p_hat = x / n
    p = alpha

    # Evitar log(0)
    if p_hat == 0 or p_hat == 1:
        return np.inf, 0.0

    lruc = -2 * (
        (x * np.log(p) + (n - x) * np.log(1 - p)) -
        (x * np.log(p_hat) + (n - x) * np.log(1 - p_hat))
    )
    p_value = 1 - chi2.cdf(lruc, df=1)
    return lruc, p_value, p_hat

def christoffersen_tests(hit_series, alpha=0.05):
    """
    hit_series: array-like (0/1), 1 = VaR violado. Must be ordered in time.
    alpha: nominal VaR level (e.g. 0.05)
    
    Returns a dict with:
      - counts: n, x, p_hat
      - transition counts: n00,n01,n10,n11
      - pi0, pi1, pi (estimated probs)
      - LR_uc, p_uc (Kupiec)
      - LR_ind, p_ind (independence)
      - LR_cc, p_cc (combined conditional coverage)
    """
    hits = np.asarray(hit_series).astype(int)
    n = len(hits)
    x = hits.sum()
    p_hat = x / n

    # --- Kupiec (LRuc) - unconditional coverage
    p = alpha
    if p_hat == 0 or p_hat == 1:
        LR_uc = np.nan
        p_uc = np.nan
    else:
        LR_uc = -2 * (
            (x * np.log(p) + (n - x) * np.log(1 - p)) -
            (x * np.log(p_hat) + (n - x) * np.log(1 - p_hat))
        )
        p_uc = 1 - chi2.cdf(LR_uc, df=1)

    # --- Transition counts for independence test
    n00 = n01 = n10 = n11 = 0
    for t in range(1, n):
        prev, cur = hits[t-1], hits[t]
        if prev == 0 and cur == 0: n00 += 1
        elif prev == 0 and cur == 1: n01 += 1
        elif prev == 1 and cur == 0: n10 += 1
        elif prev == 1 and cur == 1: n11 += 1

    # conditional probabilities
    denom0 = n00 + n01
    denom1 = n10 + n11
    pi0 = n01 / denom0 if denom0 > 0 else np.nan  # P(hit_t=1 | hit_{t-1}=0)
    pi1 = n11 / denom1 if denom1 > 0 else np.nan  # P(hit_t=1 | hit_{t-1}=1)
    pi = (n01 + n11) / (n00 + n01 + n10 + n11) if (n00 + n01 + n10 + n11) > 0 else np.nan

    # log-likelihoods for independence test
    def safe_log(x):
        return np.log(x) if x>0 else -1e10

    # Likelihood under independence (state-dependent probs)
    # L_ind = sum counts * log(probabilities)
    L_ind = 0.0
    if denom0>0:
        L_ind += n00 * safe_log(1 - pi0) + n01 * safe_log(pi0)
    if denom1>0:
        L_ind += n10 * safe_log(1 - pi1) + n11 * safe_log(pi1)

    # Likelihood under iid (unconditional prob pi)
    L_uncond = 0.0
    if (n00 + n01 + n10 + n11) > 0:
        L_uncond += (n00) * safe_log(1 - pi) + (n01) * safe_log(pi)
        L_uncond += (n10) * safe_log(1 - pi) + (n11) * safe_log(pi)

    LR_ind = -2 * (L_uncond - L_ind)
    p_ind = 1 - chi2.cdf(LR_ind, df=1)

    # Combined conditional coverage
    # LR_cc = LR_uc + LR_ind  (df = 2)
    LR_cc = LR_uc + LR_ind if (not np.isnan(LR_uc)) else np.nan
    p_cc = 1 - chi2.cdf(LR_cc, df=2) if (not np.isnan(LR_cc)) else np.nan

    out = {
        'n': n, 'x': int(x), 'p_hat': p_hat, 'alpha': alpha,
        'n00': n00, 'n01': n01, 'n10': n10, 'n11': n11,
        'pi0': pi0, 'pi1': pi1, 'pi': pi,
        'LR_uc': LR_uc, 'p_uc': p_uc,
        'LR_ind': LR_ind, 'p_ind': p_ind,
        'LR_cc': LR_cc, 'p_cc': p_cc
    }
    return out

# Obtenção dos dados que serão usados no backtest.

tickers = ['NVDA', 'MSFT', 'AAPL', 'AMZN', 'META', 'AVGO', 'GOOG', 'TSLA', 'BRK-B', 'JPM',
           'ORCL', 'LLY', 'NFLX', 'MA', 'XOM', 'INTC']

janela = 252

df_pesos = pd.read_csv('./dados/df_pesos_historicos.csv', index_col='Date')
df_pesos = df_pesos.div(df_pesos.sum(axis=1), axis=0)
df_pesos = df_pesos.iloc[1:]

df_modelo = pd.read_csv('./dados/dados_modelo.csv',index_col='Date')

df_fatores = df_modelo[['^GSPC', 'SMB', 'HML', 'MOM']]
df_tickers = df_modelo.drop(columns=['^GSPC', 'SMB', 'HML', 'MOM'])

# Obtenção dos betas das regressões e dos resíduos
# Nota-se, é utilizado, para o dia t, dados até o dia t.

betas_dict, resid_dict = {}, {}

for i in range(janela, len(df_tickers)):
    data = df_tickers.index[i]
    janela_retornos = df_tickers.iloc[0:i]
    janela_fat = df_fatores.iloc[0:i]
    betas, resid = {}, {}
    
    for ticker in tickers:
        x = sm.add_constant(janela_fat)
        y = janela_retornos[ticker]
        model = sm.OLS(y, x).fit()
        #print(model.params.values)
        betas[ticker] = model.params.values
        resid[ticker] = model.resid.var()

    betas_dict[data] = pd.DataFrame(betas).T  # tickers x fatores
    resid_dict[data] = resid

for data in betas_dict.keys():
    betas_dict[data] = betas_dict[data].iloc[:, 1:]

specific_var_dict = {}

for data in resid_dict.keys():
    resid_val = np.array([resid_dict[data][ticker] for ticker in tickers])
    
    d_t = np.diag(resid_val)

    specific_var_dict[data] = d_t

# Obtenção das covariâncias dos fatores.

sigma_f_dict = {}

for i in range(janela, len(df_tickers)):
    data = df_tickers.index[i]
    janela_fat = df_fatores.iloc[0:i]
    sigma_f = janela_fat.cov().values
    sigma_f_dict[data] = sigma_f

# Com os dados de betas, resíduos e covariância, obtem os dados do risco de portfólio
# para cada período.

port_risk_dict = {}

for i in range(janela, len(df_tickers)):
    data = df_tickers.index[i]

    w = df_pesos[df_pesos.index == data].to_numpy().flatten()
    sig = sig_total_dict[data]
    port_risk = np.sqrt(w.T @ sig @ w)

    port_risk_dict[data] = port_risk


# Assim, obtem-se os dados de VaR para cada período.

conf = 0.95
var_dict = {}
for data in port_risk_dict.keys():
    port_risk = port_risk_dict[data]
    VaR = norm.ppf(1-conf) * port_risk
    var_dict[data] = VaR


# Após isso, pega-se os dados de retorno do portfólio em si, considerando os pesos.

port_ret_dict = {}

for i in range(janela, len(df_tickers)):
    data = df_tickers.index[i]
    w = df_pesos[df_pesos.index == data].to_numpy().flatten()
    ret = df_tickers.iloc[i]
    ret_port = ret @ w
    port_ret_dict[data] = ret_port

# Com os testes feitos, descobre-se que os dados não são significativos se forem considerados em sua
# totalidade. Assim, retira-se algumas observalçoes.

df_var = pd.Series(var_dict, name='VaR')
df_var.index = pd.to_datetime(df_var.index)
df_var = df_var.sort_index()
df_var = df_var.iloc[200:]

df_ret_port = pd.Series(port_ret_dict, name='retorno_port')
df_ret_port.index = pd.to_datetime(df_ret_port.index)
df_ret_port = df_ret_port.iloc[200:]

# Usa-se os testes de Kupiec e Christoffersen para ver se o modelo está bem calibrado.

violations = (df_ret_port < df_var).astype(int)
alpha = 0.05
lruc, pval, p_hat = kupiec_test(violations, alpha)

print(f"Violations: {np.sum(violations)} de {len(violations)} dias")
print(f"Frequência observada: {p_hat:.3%}")
print(f"Esperada: {alpha:.1%}")
print(f"LRuc: {lruc:.3f}")
print(f"p-value: {pval:.3f}")
print("Resultado:", "✅ Aprovado" if pval > 0.05 else "❌ Rejeitado")

res = christoffersen_tests(violations.values, alpha=0.05)

print("N:", res['n'], "Violations:", res['x'], f"p̂={res['p_hat']:.4f}")
print("\nTransition counts: n00,n01,n10,n11 =", res['n00'], res['n01'], res['n10'], res['n11'])
print(f"pi0 = P(hit|prev=0) = {res['pi0']}, pi1 = P(hit|prev=1) = {res['pi1']}")
print("\nKupiec (LR_uc): LR = {:.3f}, p = {:.3f}".format(res['LR_uc'], res['p_uc']))
print("Independence (LR_ind): LR = {:.3f}, p = {:.3f}".format(res['LR_ind'], res['p_ind']))
print("Conditional coverage (LR_cc): LR = {:.3f}, p = {:.3f}".format(res['LR_cc'], res['p_cc']))

# Por fim, calcula-se o CVaR para cada data.

cvar_dict = {}

for i in range(0, len(df_ret_port)):
    data = df_ret_port.index[i]
    ret_port = df_ret_port.iloc[:i+1].copy()
    var_i = df_var.iloc[i]
    perdas = ret_port.loc[ret_port <= var_i]
    cvar_i = perdas.mean() if not perdas.empty else np.nan
    cvar_dict[data] = cvar_i
    
df_cvar = pd.Series(cvar_dict, name='CVaR')
df_cvar.index = pd.to_datetime(df_cvar.index)
df_cvar = df_cvar.sort_index()
df_cvar

df_cvar.to_csv('./dados/dados_cvar.csv')